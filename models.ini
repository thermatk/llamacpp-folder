#version = 1
# https://github.com/ggml-org/llama.cpp/issues/18428
[*]
n = -1

#https://huggingface.co/unsloth/GLM-4.5-Air-GGUF
[GLM-4.5-Air]
jinja = true
ctx-size = 16384
seed = 3407
fit = on

#https://unsloth.ai/docs/models/tutorials/glm-4.6-how-to-run-locally
[GLM-4.6V-Flash]
jinja = true
ctx-size = 16384
flash-attn = on
temp = 0.8
top-p = 0.6
top-k = 2
repeat-penalty = 1.1
#ot = .ffn_.*_exps.=CPU
seed = 3407
#system-prompt = Respond in English and reason in English

[GLM-4.6V]
jinja = true
ctx-size = 16384
flash-attn = on
temp = 1.0
top-p = 0.95
top-k = 40
#ot = .ffn_.*_exps.=CPU
seed = 3407
#system-prompt = Respond in English and reason in English

#https://unsloth.ai/docs/models/glm-4.7-flash
[GLM-4.7-Flash]
ctx-size = 32768
fit = on
seed = 3407
temp = 1.0
top-p = 0.95
min-p = 0.01
top-k = 64
jinja = true
#for tool calling:
#temp = 0.7
#top-p = 1.0

#https://unsloth.ai/docs/models/gemma-3-how-to-run-and-fine-tune
[gemma-3-27b-it-qat]
ctx-size = 32768
seed = 3407
prio = 2
temp = 1.0
repeat-penalty = 1.0
min-p = 0.01
top-p = 0.95
top-k = 64

[gemma-3-4b-it]
ctx-size = 32768
seed = 3407
prio = 2
temp = 1.0
repeat-penalty = 1.0
min-p = 0.00
top-p = 0.95
top-k = 64

[gemma-3-12b-it]
ctx-size = 32768
seed = 3407
prio = 2
temp = 1.0
repeat-penalty = 1.0
min-p = 0.00
top-p = 0.95
top-k = 64

#https://unsloth.ai/docs/models/gemma-3-how-to-run-and-fine-tune/gemma-3n-how-to-run-and-fine-tune
[gemma-3n-E4B-it]
ctx-size = 32768
seed = 3407
prio = 2
temp = 1.0
repeat-penalty = 1.0
min-p = 0.00
top-p = 0.95
top-k = 64

#https://huggingface.co/unsloth/Mistral-Small-3.2-24B-Instruct-2506-GGUF
[Mistral-Small-3.2-24B-Instruct-2506]
jinja = true
ctx-size = 16384
seed = 3407
temp = 0.15
top-p = 1.00
top-k = -1

#https://unsloth.ai/docs/models/tutorials/magistral-how-to-run-and-fine-tune
[Magistral-Small-2509]
jinja = true
ctx-size = 16384
seed = 3407
temp = 0.7
top-p = 0.95
top-k = -1

#https://unsloth.ai/docs/models/qwen3-vl-how-to-run-and-fine-tune
[Qwen3-VL-8B-Instruct]
jinja = true
temp = 0.7
top-p = 0.8
top-k = 20
min-p = 0.0
presence-penalty = 1.5
ctx-size = 32768

#
[Qwen3-VL-30B-A3B-Thinking]
jinja = true
temp = 1.0
top-p = 0.95
top-k = 20
min-p = 0.0
presence-penalty = 0.0
ctx-size = 32768

#
[Qwen3-VL-32B-Instruct]
jinja = true
temp = 0.7
top-p = 0.8
top-k = 20
min-p = 0.0
presence-penalty = 1.5
ctx-size = 32768

[Qwen3-VL-4B-Thinking]
jinja = true
temp = 1.0
top-p = 0.95
top-k = 20
min-p = 0.0
presence-penalty = 0.0
ctx-size = 32768

#https://unsloth.ai/docs/models/qwen3-how-to-run-and-fine-tune/qwen3-2507#llama.cpp-run-qwen3-30b-a3b-instruct-2507-tutorial
[Qwen3-30B-A3B-Instruct-2507]
jinja = true
temp = 0.7
min-p = 0.0
top-p = 0.80
top-k = 20
presence-penalty = 1.0
ctx-size = 32768

#https://unsloth.ai/docs/models/tutorials/qwen3-next
[Qwen3-Next-80B-A3B-Thinking]
jinja = true
temp = 0.6
min-p = 0.0
top-p = 0.95
top-k = 20
presence-penalty = 1.0
ctx-size = 32768

#https://unsloth.ai/docs/models/qwen3-coder-next
[Qwen3-Coder-Next]
jinja = true
fit = on
seed = 3407
temp = 1.0
top-p = 0.95
min-p = 0.01
top-k = 40
ctx-size = 262144

#https://github.com/ggml-org/llama.cpp/discussions/15396
#https://unsloth.ai/docs/models/gpt-oss-how-to-run-and-fine-tune#running-gpt-oss
#The reasoning level can be set in the system prompts, e.g., "Reasoning: high". or Medium or Low
[gpt-oss-20b]
jinja = true
ctx-size = 131072
flash-attn = on
ub = 2048
b = 2048
temp = 1.0
top-p = 1.0

[gpt-oss-120b]
jinja = true
ctx-size = 16384
flash-attn = on
ub = 2048
b = 2048
temp = 1.0
top-p = 1.0
#ot = .ffn_.*_exps.=CPU

#https://huggingface.co/mradermacher/gpt-oss-20b-heretic-v2-i1-GGUF
#https://github.com/p-e-w/heretic
[gpt-oss-20b-heretic-v2]
jinja = true
ctx-size = 131072
flash-attn = on
ub = 2048
b = 2048
temp = 1.0
top-p = 1.0

#https://huggingface.co/bartowski/kldzj_gpt-oss-120b-heretic-v2-GGUF
[kldzj_gpt-oss-120b-heretic-v2]
jinja = true
ctx-size = 16384
flash-attn = on
ub = 2048
b = 2048
temp = 1.0
top-p = 1.0

#https://huggingface.co/ibm-granite/granite-docling-258M
#https://huggingface.co/ggml-org/granite-docling-258M-GGUF
#https://huggingface.co/ibm-granite/granite-docling-258M-GGUF
#https://docling-project.github.io/docling/getting_started/rtx/#windows-using-llama-server
[granite-docling-258M]
cb = true
context-shift = true
ctx-size = 131072
np = 16

#https://huggingface.co/gpustack/bge-m3-GGUF
#https://huggingface.co/BAAI/bge-m3
[bge-m3]
ctx-size = 8192
embeddings = true
